{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extending the capabilities of our model\n",
    "\n",
    "An LLM is a very capable tool, but only to the extent of the knowledge or information it has been trained on. After all, you only know what you know, right? But what if you need to ask a question that is not in the training data? Or what if you need to ask a question that is not in the training data, but is related to it?\n",
    "\n",
    "There are different ways to solve this problem, depending on the resources you have and the time or money you can spend on it. Here are a few options:\n",
    "\n",
    "- Fully retrain the model to include the information you need. For an LLM, it's only possible for a handful of companies in the world that can afford literally thousands of GPUs running for weeks.\n",
    "- Fine-tune the model with this new information. This requires way less resources, and can usually be done in a few hours or minutes (depending on the size of the model). However as it does not fully retrain the model, the new information may not be completely integrated in the answers. Fine-tuning excels at giving a better understanding of a specific context or vocabulary, a little bit less on injecting new knowledge. Plus you have to retrain and redeploy the model anyway any time you want to add more information.\n",
    "- Put this new information in a database and have the parts relevant to the query retrieved and added to this query as a context before sending it to the LLM. This technique is called **Retrieval Augmented Generation, or RAG**. It is interesting as you don't have to retrain or fine-tune the model to benefit of this new knowledge, that you can easily update at any time.\n",
    "\n",
    "We have already prepared a Vector Database using [Azure AI Search](https://azure.microsoft.com/en-us/products/ai-services/ai-search), where we have stored the content of the [California Driver's Handbook](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/).\n",
    "\n",
    "In this Notebook, we are going to use RAG to **make some queries about a Claim** and see how this new knowledge can help without having to modify our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Library imports\n",
    "\n",
    "First we will import the libraries we need, they are already installed on our workbench image so no need to run `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    "    CosmosDBVectorSearchType\n",
    ")\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ClientAuthenticationError\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "    print(\"Found Azure AI Search instance: \" + os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\"))\n",
    "else:\n",
    "    print(\".env file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain elements\n",
    "\n",
    "Again, we are going to use Langchain to define our task pipeline.\n",
    "\n",
    "First, the **LLM** where we will send our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    credential = DefaultAzureCredential()\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    os.environ[\"AZURE_OPENAI_AD_TOKEN\"] = access_token.token\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_DEPLOYMENT\"),\n",
    "    max_tokens=512,\n",
    "    temperature=0.01,\n",
    "    top_p=0.92,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "Then the connection to the **vector database** where we have prepared and stored the California Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we define the embeddings that we used to process the Handbook\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment = os.getenv(\"AZURE_EMBEDDING\")\n",
    ")\n",
    "\n",
    "azure_ai_search_service_name = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
    "azure_ai_search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\")\n",
    "\n",
    "if not os.getenv(\"AZURE_AI_SEARCH_API_KEY\"):\n",
    "    search_access_token = credential.get_token(\"https://search.azure.com/.default\")\n",
    "    os.environ[\"AZURE_AI_SEARCH_AD_TOKEN\"] = search_access_token.token\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    content_key=\"chunk\", top_k=5, index_name=azure_ai_search_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We will now define the **template** to use to make our query. Note that this template now contains a **References** section. That's were the documents returned from the vector database will be injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant named \"Parasol Assistant\".\n",
    "You will be given a claim summary, references to provide you with information, and a question.\n",
    "You must answer the question based as much as possible on this claim with the help of the references.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Claim Summary:\n",
    "{claim}\n",
    "\n",
    "References:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!\n",
    "\n",
    "In the `claims` folder we have JSON files with examples of claims that could be received. We are going to read the first claim and ask a question related to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the claim and put its content in the \"claim\" variable\n",
    "\n",
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# Opening JSON file\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claim = data[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cde40-3571-4e3c-9b05-78389765c98f",
   "metadata": {},
   "source": [
    "### First test, no additional knowledge\n",
    "\n",
    "Let's start with a first query about the claim, but without help from our vector database.\n",
    "\n",
    "> Note: You may see a warning message about `LLMChain` being deprecated, you can ignore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac398b4-d555-45e5-aab9-d9b319f07108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and send our query.\n",
    "\n",
    "query = \"Was Daniel allowed to pass at the red light?\"\n",
    "\n",
    "# Quick hack to reuse the same template with a different type of query.\n",
    "prompt_template = template.format(claim=claim).format(context=\"\", question=query)\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "conversation = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "resp = conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f714d-c6e7-4220-a16b-fc65dbae91fb",
   "metadata": {},
   "source": [
    "We can see that the answer is valid. Here the model is using its general understanding of traffic regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e9a93-9b81-424a-96a9-f447e417c8c1",
   "metadata": {},
   "source": [
    "### Second test, with added knowledge\n",
    "\n",
    "We will use the same prompt and query, but this time the model will have access to some references from the California's Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and send our query.\n",
    "\n",
    "query = \"Was Daniel allowed to pass at the red light?\"\n",
    "\n",
    "prompt_template = template.format(claim=claim)\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "resp = rag_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5659f0-a27f-4b9e-8dd1-e05f37671c8f",
   "metadata": {},
   "source": [
    "That is pretty neat! Now the model refers directly to a source stating that **a red traffic signal light means \"STOP.\"**.\n",
    "\n",
    "But where did we get this information from? We can look into the sources associated with the answers from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e424c52-9a1a-4425-a105-4e0744ec0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_pages_number(input_string):\n",
    "    # Use regular expression to find the \"pages_\" followed by digits\n",
    "    match = re.search(r'pages_(\\d+)', input_string)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def format_sources(input_list):\n",
    "    sources = []\n",
    "\n",
    "    # Define fixed widths for the columns\n",
    "    page_number_width = 4\n",
    "    page_content_width = 90\n",
    "\n",
    "    # Add header row\n",
    "    header = f\"| {'Page':<{page_number_width}} | {'Page content snippet':<{page_content_width}} |\"\n",
    "    sources.append(header)\n",
    "    sources.append('-' * len(header))  # Add a separator line\n",
    "\n",
    "    for item in input_list:\n",
    "        pages_number = extract_pages_number(item.metadata[\"chunk_id\"])\n",
    "        page_content_preview = item.page_content.replace('\\r', '').replace('\\n', '')[:80] + \"...\"\n",
    "        # Format the string with fixed column widths\n",
    "        formatted_string = f\"| {pages_number:<{page_number_width}} | {page_content_preview:<{page_content_width}} |\"\n",
    "        sources.append(formatted_string)\n",
    "    return sources\n",
    "\n",
    "results = format_sources(resp['source_documents'])\n",
    "\n",
    "for line in results:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "That's it! We now know how to complement our LLM with some external knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
